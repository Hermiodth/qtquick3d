/****************************************************************************
**
** Copyright (C) 2014 NVIDIA Corporation.
** Copyright (C) 2019 The Qt Company Ltd.
** Contact: https://www.qt.io/licensing/
**
** This file is part of Qt 3D Studio.
**
** $QT_BEGIN_LICENSE:GPL$
** Commercial License Usage
** Licensees holding valid commercial Qt licenses may use this file in
** accordance with the commercial license agreement provided with the
** Software or, alternatively, in accordance with the terms contained in
** a written agreement between you and The Qt Company. For licensing terms
** and conditions see https://www.qt.io/terms-conditions. For further
** information use the contact form at https://www.qt.io/contact-us.
**
** GNU General Public License Usage
** Alternatively, this file may be used under the terms of the GNU
** General Public License version 3 or (at your option) any later version
** approved by the KDE Free Qt Foundation. The licenses are as published by
** the Free Software Foundation and appearing in the file LICENSE.GPL3
** included in the packaging of this file. Please review the following
** information to ensure the GNU General Public License requirements will
** be met: https://www.gnu.org/licenses/gpl-3.0.html.
**
** $QT_END_LICENSE$
**
****************************************************************************/

#ifndef SAMPLE_PROBE_GLSLLIB
#define SAMPLE_PROBE_GLSLLIB 1

#ifdef QQ3D_SHADER_META
/*{
    "uniforms": [
        { "type": "sampler2D", "name": "qt_lightProbe" },
        { "type": "vec4", "name": "qt_lightProbeProperties" },
        { "type": "vec4", "name": "qt_lightProbeRotation" },
        { "type": "vec4", "name": "qt_lightProbeOffset " },
        { "type": "vec4", "name": "qt_lightProbeOptions", "condition": "QSSG_ENABLE_IBL_FOV" },
        { "type": "mat4", "name": "qt_lightProbeOrientation", "condition": "QSSG_ENABLE_IBL_ORIENTATION" }
    ]
}*/
#endif // QQ3D_SHADER_META

vec3 qt_textureProbe(sampler2D lightProbe, vec2 coord, float lod)
{
#ifdef QSSG_ENABLE_RGBE_LIGHT_PROBE
    vec4 ret = textureLod(lightProbe, coord, lod);
    return ret.rgb * pow(2.0, ret.a * 255.0 - 128.0);
#else
    return textureLod(lightProbe, coord, lod).rgb;
#endif
}

vec2 qt_transformSample(vec2 origUV, vec4 probeRot, vec2 probeOfs)
{
    vec2 retUV;
    retUV.x = dot(vec3(origUV, 1.0), vec3(probeRot.xy, probeOfs.x));
    retUV.y = dot(vec3(origUV, 1.0), vec3(probeRot.zw, probeOfs.y));
    return retUV;
}

// This is broken out into its own routine so that if we get some other
// format image than a lat-long, then we can account for that by changing
// the code here alone.
vec2 qt_getProbeSampleUV(vec3 smpDir, vec4 probeRot, vec2 probeOfs)
{
    vec2 smpUV;
#if QSSG_ENABLE_IBL_ORIENTATION
    smpDir = mat3(qt_lightProbeOrientation) * smpDir;
#endif

#if QSSG_ENABLE_IBL_FOV
    smpUV = vec2(atan(smpDir.x, smpDir.z), asin(smpDir.y));
    // assume equirectangular HDR spherical map (instead of cube map) and warp sample
    // UV coordinates accordingly
    smpUV *= 2.0 * vec2(0.1591596371160, 0.318319274232054);
    // Default FOV is 180 deg = pi rad. Narrow the FOV
    // by scaling texture coordinates by the ratio of
    // incoming FOV to 180 degrees default
    smpUV *= 3.14159265358 / qt_lightProbeOptions.x;
#else
    smpUV.x = atan( smpDir.x, smpDir.z) / 3.14159265359;
    smpUV.y = 1.0 - (acos(smpDir.y) / 1.57079632679);
#endif
    smpUV = qt_transformSample( smpUV.xy * 0.5, probeRot, probeOfs ) + vec2(0.5, 0.5);

    return smpUV;
}

vec3 qt_getProbeSample(vec3 smpDir, float lodShift, vec3 normal)
{
    vec2 smpUV = qt_getProbeSampleUV(smpDir, qt_lightProbeRotation, qt_lightProbeOffset.xy);
    return qt_textureProbe(qt_lightProbe, smpUV, lodShift);
}

vec4 qt_sampleDiffuse(mat3 tanFrame)
{
    // Note: qt_lightProbeProperties.w = Exposure
    if (qt_lightProbeProperties.w < 0.005)
        return vec4(0.0);
    vec2 smpUV = qt_getProbeSampleUV(tanFrame[2], qt_lightProbeRotation, qt_lightProbeOffset.xy);
    const float d = 0.761324705; // 1.0 / 1.3125;
    float baseLevel = qt_lightProbeOffset.w - 1.0;
    vec3 val = qt_textureProbe(qt_lightProbe, smpUV, max(baseLevel - 2.0, 0.0)) * 0.0625;
    val += qt_textureProbe(qt_lightProbe, smpUV, max(baseLevel - 1.0, 0.0)) * 0.25;
    val += qt_textureProbe(qt_lightProbe, smpUV, max(baseLevel, 0.0));

    if (qt_lightProbeProperties.z > -1.0) {
        float ctr = 0.5 + 0.5 * qt_lightProbeProperties.z;
        float vertWt = smoothstep(ctr * 0.25, ctr + 0.25, smpUV.y);
        float wtScaled = mix(1.0, vertWt, qt_lightProbeProperties.z + 1.0);
        val *= wtScaled;
    }

    return vec4(d * qt_lightProbeProperties.w * val, 1.0);
}

// This method is used by DefaultMaterial for the specular term
vec4 qt_sampleGlossy(mat3 tanFrame, vec3 viewDir, float rough)
{
    // Note: qt_lightProbeProperties.w == Exposure
    if (qt_lightProbeProperties.w < 0.005)
        return vec4(0.0);

    float sigma = smoothstep(0.0, 1.0, clamp(rough, 0.0001, 1.0));
    vec3 ret = vec3(0, 0, 0);
    vec3 smpDir = reflect(-viewDir, tanFrame[2]);

    // Compute the Geometric occlusion/self-shadowing term
    float NdotL = clamp(dot(smpDir, tanFrame[2]), 0.0, 0.999995);
    float k = sigma * 0.31830988618;    // roughness / pi
    float Gl = clamp((NdotL / (NdotL*(1.0-k) + k) + (1.0 - k*k)) * 0.5, 0.0, 1.0 );
    float levels = log2(max(vec2(textureSize(qt_lightProbe, 0)).x, vec2(textureSize(qt_lightProbe, 0)).y));

    vec3 outColor;

    outColor = qt_getProbeSample(smpDir, sigma * levels, tanFrame[2]);

    return vec4(qt_lightProbeProperties.w * Gl * outColor, 1.0);
}

// An alternative to using a LUT is to use this function
// see: https://www.unrealengine.com/en-US/blog/physically-based-shading-on-mobile
vec2 qt_brdfApproximation(in vec3 N, in vec3 viewDir, in float roughness)
{
    float nDotV = clamp(dot(N, viewDir), 0.0, 1.0);
    const vec4 c0 = vec4(-1.0, -0.0275, -0.572, 0.022);
    const vec4 c1 = vec4(1.0, 0.0425, 1.04, -0.04);
    vec4 r = roughness * c0 + c1;
    float a004 = min(r.x * r.x, exp2(-9.28 * nDotV)) * r.x + r.y;
    return vec2(-1.04, 1.04) * a004 + r.zw;
}

// This method is used by the PrincipledMaterial IBL specular term to adjust
// which LOD is used from the convolved mip map levels. Using a linear curve
// just based on the roughness level doesn't map well to what is expected, so
// this is a quadradic curve that favors more higher mip levels in the mid
// roughness range.  If we choose to use more a more "standard" way of
// environment mapping it will no longer be necessary to adjust the roughness.
float qt_adjustRoughness(float roughness)
{
    const vec2 a = vec2(-0.7, 0.7) * 0.45 + 0.5;
    const float om2a = 1.0 - 2.0 * a.x;
    float t = (sqrt(a.x * a.x + om2a * roughness) - a.x) / om2a;
    float y = (1.0 - 2.0 * a.y) * (t * t) + (2.0 * a.y) * t;
    return y;
}

// This method is used by the PrincipledMaterial for the IBL specular term.
vec4 qt_sampleGlossyPrincipled(mat3 tanFrame, vec3 viewDir, vec3 F, float roughness)
{
    if (qt_lightProbeProperties.w < 0.005)
        return vec4(0.0);

    float levels = log2(max(vec2(textureSize(qt_lightProbe, 0)).x, vec2(textureSize(qt_lightProbe, 0)).y));

    roughness = qt_adjustRoughness(roughness);

    float lod = clamp((roughness * levels), 0.0, levels);
    vec3 smpDir = normalize(reflect(-viewDir, tanFrame[2]));

    vec3 specularSample = qt_getProbeSample(smpDir, lod, tanFrame[2]);
    vec2 brdf = qt_brdfApproximation(tanFrame[2], viewDir, roughness);

    return vec4(specularSample * (F * brdf.x + brdf.y), 1.0);
}

#endif
